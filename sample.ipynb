{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f39658c2ea385d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T07:41:25.508229Z",
     "start_time": "2024-04-04T07:41:25.503950Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "file_path = 'ИТМО_кейс_обучающая_выборка.csv'\n",
    "data = pd.read_csv(file_path)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T07:41:22.038649Z",
     "start_time": "2024-04-04T07:41:11.505612Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                    0\n",
      "Задача                        0\n",
      "Обстановка                    0\n",
      "Оптимальный план              0\n",
      "Задача en                     0\n",
      "Обстановка en                 0\n",
      "Оптимальный план en           0\n",
      "Предсказанный план            0\n",
      "Успех предсказанного плана    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T07:41:27.121949Z",
     "start_time": "2024-04-04T07:41:27.111984Z"
    }
   },
   "id": "4f83d29547bd4a37",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                         Задача  \\\n",
      "0           0  Приготовить горячий шоколад.    \n",
      "1           1    Заказать доставку еды домой   \n",
      "2           2   Реставрация старого комода\\n   \n",
      "3           3           Приготовить сырники.   \n",
      "4           4              Нарезать колбасу.   \n",
      "\n",
      "                                          Обстановка  \\\n",
      "0  кухня, молоко, какао, сахар, кастрюля, ложка, ...   \n",
      "1  мобильный телефон, ресторанное меню, интернет,...   \n",
      "2  Старый комод, повреждения, шпатель, шкурка, ма...   \n",
      "3  Кухня, столешница, творог, мука, яйцо, сахар, ...   \n",
      "4  Кухня, колбаса, нож, доска для резки, холодиль...   \n",
      "\n",
      "                                    Оптимальный план  \\\n",
      "0  Откройте дверь. Зайдите на кухню. Закрйоте две...   \n",
      "1  Найдите стол. Подойдите к столу. Найдите мобил...   \n",
      "2  Подойдите к комоду.Найдите шкурку.Возьмите шку...   \n",
      "3  Подойти к столешнице. Найти творог. Взять твор...   \n",
      "4  Подойти к холодильнику. Открыть холодильник. Н...   \n",
      "\n",
      "                                Задача en  \\\n",
      "0                  Prepare hot chocolate.   \n",
      "1        Order food delivery to your home   \n",
      "2  Restoration of an old chest of drawers   \n",
      "3                    Prepare cheesecakes.   \n",
      "4                        Cut the sausage.   \n",
      "\n",
      "                                       Обстановка en  \\\n",
      "0  kitchen, milk, cocoa, sugar, pan, spoon, cabin...   \n",
      "1  mobile phone, restaurant menu, internet, money...   \n",
      "2  Old chest of drawers, damage, spatula, sandpap...   \n",
      "3  Kitchen, countertop, cottage cheese, flour, eg...   \n",
      "4  Kitchen, sausage, knife, cutting board, refrig...   \n",
      "\n",
      "                                 Оптимальный план en  \\\n",
      "0  Open the door. Walk to the kitchen. Close the ...   \n",
      "1  Find a table. Come to the table. Find a mobile...   \n",
      "2  Walk to the chest of drawers. Find the sandpap...   \n",
      "3  Walk to the tabletop. Find cottage cheese. Gra...   \n",
      "4  Walk to the refrigerator. Open the refrigerato...   \n",
      "\n",
      "                                  Предсказанный план  \\\n",
      "0    Walk to the closet. Open the closet. Find a ...   \n",
      "1    Come to the table. Find your phone. Grab the...   \n",
      "2    Find the chest of drawers. Walk to the chest...   \n",
      "3    Walk to the tabletop. Find a bowl. Grab a bo...   \n",
      "4    Walk to the refrigerator. Open the refrigera...   \n",
      "\n",
      "   Успех предсказанного плана  \n",
      "0                           0  \n",
      "1                           1  \n",
      "2                           0  \n",
      "3                           0  \n",
      "4                           1  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T07:41:40.177283Z",
     "start_time": "2024-04-04T07:41:40.169021Z"
    }
   },
   "id": "ab427f19809c9a35",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Успех предсказанного плана\n",
      "1    3234\n",
      "0    1600\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['Успех предсказанного плана'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T07:41:50.748368Z",
     "start_time": "2024-04-04T07:41:50.740943Z"
    }
   },
   "id": "f3bc9d36608379a3",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  Успех предсказанного плана\n",
      "count  4834.000000                 4834.000000\n",
      "mean   2416.500000                    0.669011\n",
      "std    1395.599931                    0.470618\n",
      "min       0.000000                    0.000000\n",
      "25%    1208.250000                    0.000000\n",
      "50%    2416.500000                    1.000000\n",
      "75%    3624.750000                    1.000000\n",
      "max    4833.000000                    1.000000\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T07:42:04.484648Z",
     "start_time": "2024-04-04T07:42:04.470754Z"
    }
   },
   "id": "f0c314486e8efdf9",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T07:43:42.461264Z",
     "start_time": "2024-04-04T07:43:28.784752Z"
    }
   },
   "id": "1c923bcde62c36a7",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['Задача'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T07:43:44.053589Z",
     "start_time": "2024-04-04T07:43:43.993146Z"
    }
   },
   "id": "dc8ed34120b864ec",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4834, 5402)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T07:43:45.332485Z",
     "start_time": "2024-04-04T07:43:45.328127Z"
    }
   },
   "id": "80d812178b855cd1",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zamotaevrodion/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zamotaevrodion/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/zamotaevrodion/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/PY3/english.pickle\u001B[0m\n\n  Searched in:\n    - '/Users/zamotaevrodion/nltk_data'\n    - '/Users/zamotaevrodion/opt/anaconda3/envs/case_for_olimp/nltk_data'\n    - '/Users/zamotaevrodion/opt/anaconda3/envs/case_for_olimp/share/nltk_data'\n    - '/Users/zamotaevrodion/opt/anaconda3/envs/case_for_olimp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m     words \u001B[38;5;241m=\u001B[39m [lemmatizer\u001B[38;5;241m.\u001B[39mlemmatize(word\u001B[38;5;241m.\u001B[39mlower()) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words \u001B[38;5;28;01mif\u001B[39;00m word\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words \u001B[38;5;129;01mand\u001B[39;00m word\u001B[38;5;241m.\u001B[39misalpha()]\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(words)\n\u001B[0;32m---> 19\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mЗадача\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mЗадача\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocess_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mОписание\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mОписание обстановки\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(preprocess_text)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/pandas/core/series.py:4630\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[1;32m   4520\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4521\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4522\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4525\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4526\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4527\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4528\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4529\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4628\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4629\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4630\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;66;03m# self.f is Callable\u001B[39;00m\n\u001B[0;32m-> 1025\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1074\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1075\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m-> 1076\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1077\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1078\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1079\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1080\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1082\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1083\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1084\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1085\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[12], line 15\u001B[0m, in \u001B[0;36mpreprocess_text\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreprocess_text\u001B[39m(text):\n\u001B[0;32m---> 15\u001B[0m     words \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     words \u001B[38;5;241m=\u001B[39m [lemmatizer\u001B[38;5;241m.\u001B[39mlemmatize(word\u001B[38;5;241m.\u001B[39mlower()) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words \u001B[38;5;28;01mif\u001B[39;00m word\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words \u001B[38;5;129;01mand\u001B[39;00m word\u001B[38;5;241m.\u001B[39misalpha()]\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(words)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/nltk/tokenize/__init__.py:129\u001B[0m, in \u001B[0;36mword_tokenize\u001B[0;34m(text, language, preserve_line)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mword_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m, preserve_line\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    115\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03m    Return a tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;124;03m    :type preserve_line: bool\u001B[39;00m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 129\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m \u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m    131\u001B[0m         token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m _treebank_word_tokenizer\u001B[38;5;241m.\u001B[39mtokenize(sent)\n\u001B[1;32m    132\u001B[0m     ]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/nltk/tokenize/__init__.py:106\u001B[0m, in \u001B[0;36msent_tokenize\u001B[0;34m(text, language)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msent_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     97\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m    :param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 106\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokenizers/punkt/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlanguage\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.pickle\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mtokenize(text)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/nltk/data.py:750\u001B[0m, in \u001B[0;36mload\u001B[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001B[0m\n\u001B[1;32m    747\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<<Loading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresource_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m>>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    749\u001B[0m \u001B[38;5;66;03m# Load the resource.\u001B[39;00m\n\u001B[0;32m--> 750\u001B[0m opened_resource \u001B[38;5;241m=\u001B[39m \u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresource_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    753\u001B[0m     resource_val \u001B[38;5;241m=\u001B[39m opened_resource\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/nltk/data.py:876\u001B[0m, in \u001B[0;36m_open\u001B[0;34m(resource_url)\u001B[0m\n\u001B[1;32m    873\u001B[0m protocol, path_ \u001B[38;5;241m=\u001B[39m split_resource_url(resource_url)\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m protocol \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnltk\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 876\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mopen()\n\u001B[1;32m    877\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    878\u001B[0m     \u001B[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001B[39;00m\n\u001B[1;32m    879\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m find(path_, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mopen()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/nltk/data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    581\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[1;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/PY3/english.pickle\u001B[0m\n\n  Searched in:\n    - '/Users/zamotaevrodion/nltk_data'\n    - '/Users/zamotaevrodion/opt/anaconda3/envs/case_for_olimp/nltk_data'\n    - '/Users/zamotaevrodion/opt/anaconda3/envs/case_for_olimp/share/nltk_data'\n    - '/Users/zamotaevrodion/opt/anaconda3/envs/case_for_olimp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['Задача'] = data['Задача'].apply(preprocess_text)\n",
    "data['Описание'] = data['Описание обстановки'].apply(preprocess_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T08:00:52.693546Z",
     "start_time": "2024-04-04T08:00:52.486625Z"
    }
   },
   "id": "f9b1cbe4d757cb40",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Задача_processed'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3652\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3653\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Задача_processed'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m tfidf_vectorizer \u001B[38;5;241m=\u001B[39m TfidfVectorizer(max_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5000\u001B[39m) \n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Объединяем текстовые данные для векторизации\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m combined_text \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mЗадача_processed\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mОписание_processed\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      6\u001B[0m X \u001B[38;5;241m=\u001B[39m tfidf_vectorizer\u001B[38;5;241m.\u001B[39mfit_transform(combined_text)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Целевая переменная\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3760\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3761\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3763\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/case_for_olimp/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3653\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3655\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3656\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3657\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3658\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3659\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3660\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Задача_processed'"
     ]
    }
   ],
   "source": [
    "# Инициализация векторизатора\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000) \n",
    "\n",
    "# Объединяем текстовые данные для векторизации\n",
    "combined_text = data['Задача_processed'] + \" \" + data['Описание_processed']\n",
    "X = tfidf_vectorizer.fit_transform(combined_text)\n",
    "\n",
    "# Целевая переменная\n",
    "y = data['Успех предсказанного плана']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T07:53:52.204800Z",
     "start_time": "2024-04-04T07:53:50.341638Z"
    }
   },
   "id": "98137111a26ab3f5",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1017c0c2faaf512"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of the model: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe5e1566521b8abe"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "((3867, 1000), (967, 1000))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "# Объединение текстовых полей\n",
    "data['combined_text'] = data['Задача en'] + ' ' + data['Обстановка en'] + ' ' + data['Предсказанный план']\n",
    "\n",
    "# Функция для очистки и нормализации текста\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Приведение к нижнему регистру\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Удаление лишних пробелов\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаление специальных символов\n",
    "    return text\n",
    "\n",
    "# Применение функции очистки к каждой строке объединенного текстового поля\n",
    "data['cleaned_text'] = data['combined_text'].apply(clean_text)\n",
    "\n",
    "# Подготовка данных и целевой переменной\n",
    "X = data['cleaned_text']\n",
    "y = data['Успех предсказанного плана']\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Векторизация текста с использованием TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Ограничение на количество признаков для упрощения модели\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "X_train_tfidf.shape, X_test_tfidf.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T08:17:33.476069Z",
     "start_time": "2024-04-04T08:17:32.728896Z"
    }
   },
   "id": "456a168a5a40cd28",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.6763185108583247"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Обучение модели логистической регрессии\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Предсказание на тестовой выборке\n",
    "y_pred = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "# Расчет точности модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T08:17:48.111283Z",
     "start_time": "2024-04-04T08:17:48.018991Z"
    }
   },
   "id": "8dccc411b65807b6",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "70648ff90d0c0b8a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
